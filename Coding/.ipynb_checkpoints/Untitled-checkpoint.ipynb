{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff130872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from time import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class data_ingest:\n",
    "    \n",
    "    def clean_text(self,sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(\"[^a-z]+\", \" \", sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [s for s in sentence if len(s)>1]\n",
    "        sentence = \" \".join(sentence)\n",
    "        return sentence\n",
    "\n",
    "    def caption_read(self,path):\n",
    "        '''\n",
    "        path : the location of text file which contains all the captions for the images\n",
    "        '''\n",
    "        with open (path) as f:\n",
    "            captions = f.read()\n",
    "            captions = captions.split('\\n')[1:-1]\n",
    "        \n",
    "        img_caption_dict = {}\n",
    "        \n",
    "        for caption in captions:\n",
    "            img_name, caption_for_img = caption.split('.jpg')\n",
    "            \n",
    "            if img_caption_dict.get(img_name) is None:\n",
    "                img_caption_dict[img_name] = []\n",
    "        \n",
    "            img_caption_dict[img_name].append(caption_for_img)\n",
    "        \n",
    "        for img, caption_list in img_caption_dict.items():\n",
    "            for i in range(len(caption_list)):\n",
    "                caption_list[i] = self.clean_text(caption_list[i])\n",
    "            \n",
    "        return img_caption_dict\n",
    "\n",
    "    def write_clean_captions(self, name, path):\n",
    "        '''\n",
    "        name: provide name of.txt file for writing for back up.\n",
    "        path: the location of text file which contains all the captions for the images\n",
    "        '''\n",
    "        with open(name,'w') as f:\n",
    "            f.write(str(self.caption_read(path)))\n",
    "        \n",
    "    def read_data_for_vocab(self,path):\n",
    "        '''\n",
    "        provide path for .txt file which we have written for back up.\n",
    "        '''\n",
    "        caption_text = None\n",
    "        with open (path,'r') as f:\n",
    "            caption_text = f.read()\n",
    "            jason_accepted_caption_text = caption_text.replace(\"'\",\"\\\"\")\n",
    "            caption_text = json.loads(jason_accepted_caption_text)\n",
    "        \n",
    "        # creating vocabulary with unique words\n",
    "        vocab = set()\n",
    "        for key in caption_text.keys():\n",
    "            [vocab.update(sentence.split()) for sentence in caption_text[key]]\n",
    "        \n",
    "        \n",
    "        total_words  = []\n",
    "    \n",
    "        for key in caption_text.keys():\n",
    "            [total_words.append(i) for sentence in caption_text[key] for i in sentence.split()]\n",
    "        \n",
    "        freq_dict = dict(collections.Counter(total_words))\n",
    "        threshold = 10\n",
    "        sorted_freq_dict = sorted(freq_dict.items(), reverse=True, key = lambda x : x[1])\n",
    "        final_sorted_words_with_threshold = [x[0] for x in sorted_freq_dict if x[1]>threshold]\n",
    "        \n",
    "        return vocab, final_sorted_words_with_threshold    \n",
    "        \n",
    "    # In this project data is already splitted, so we are reading the data files.\n",
    "    def train_test_dev_data(self, path1, path2, path3):\n",
    "        with open (path1) as f:\n",
    "            captions = f.read()\n",
    "            captions = captions.split('\\n')\n",
    "            train_data = [row[:-4] for row in captions][:-1]  \n",
    "    \n",
    "        with open (path2) as f:\n",
    "            captions = f.read()\n",
    "            captions = captions.split('\\n')\n",
    "            test_data = [row[:-4] for row in captions][:-1]  \n",
    "    \n",
    "        with open (path3) as f:\n",
    "            captions = f.read()\n",
    "            captions = captions.split('\\n')\n",
    "            dev_data = [row[:-4] for row in captions][:-1]  \n",
    "        \n",
    "        return train_data, test_data, dev_data\n",
    "\n",
    "    # Add <s> and <e> token to our training data\n",
    "    def train_descriptions(self, train_data, img_caption_dict):\n",
    "        train_description = {}\n",
    "        for img_id in train_data:\n",
    "            train_description[img_id] = []\n",
    "            for cap in img_caption_dict[img_id]:\n",
    "                cap_to_append = \"startseq \" + cap + \" endseq\"\n",
    "                train_description[img_id].append(cap_to_append)\n",
    "        \n",
    "        max_length = 0\n",
    "        for key in train_description.keys():\n",
    "            for i in range(len(train_description[key])):\n",
    "                temp = len((train_description[key][i]).split())\n",
    "                if temp > max_length:\n",
    "                    max_length = temp\n",
    "    \n",
    "        \n",
    "        return train_description, max_length\n",
    "    \n",
    "    # Function to convert image to feature vector\n",
    "    def encode_img(self, img_path):\n",
    "        \"\"\"\n",
    "        img_path: provide image path for preprocessing the image\n",
    "        \"\"\"\n",
    "        model = ResNet50V2(weights=\"imagenet\", input_shape= (224,224,3))\n",
    "        model_new = Model(model.input, model.layers[-2].output)\n",
    "        \n",
    "        img = image.load_img(img_path, target_size = (224,224))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        img = preprocess_input(img)\n",
    "        \n",
    "        feature_vector = model_new.predict(img).reshape((-1,))\n",
    "        return feature_vector\n",
    "    \n",
    "    def encode_data(self, data, img_folder_path):\n",
    "        \"\"\"\n",
    "        data: It should be in the format same as output from function train_test_dev_data\n",
    "        img_folder_path: It should be folder which contains all images\n",
    "        \"\"\"\n",
    "        start_t = time()\n",
    "        encoding_data = {}\n",
    "    \n",
    "        for ix, img_id in enumerate(data):\n",
    "            img_path = img_folder_path + img_id + '.jpg'\n",
    "            encoding_data[img_id] = self.encode_img(img_path)\n",
    "            if ix%100 == 0:\n",
    "                print(\"Encoding in progress with current id step {}\".format(ix))\n",
    "        end_t = time()\n",
    "        print(\"Total time taken : \", end_t - start_t)\n",
    "    \n",
    "        return encoding_data\n",
    "\n",
    "    def writing_encoded_data(self, file_name, data, img_folder_path):\n",
    "        \"\"\"\n",
    "        file_name: Provide suitable file name with .pkl format\n",
    "        data: It should be in the format same as output from function train_test_dev_data\n",
    "        img_folder_path: It should be folder which contains all images\n",
    "        \"\"\"\n",
    "        encoding_data = self.encode_data(data, img_folder_path)\n",
    "        with open (file_name, \"wb\") as f:\n",
    "            pickle.dump(encoding_data, f)\n",
    "\n",
    "    def encoding_dict_from_pickle(self, path):\n",
    "        encoding_dict_list = []\n",
    "    \n",
    "        with (open(path, \"rb\")) as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    encoding_dict_list.append(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "        encoding_dict = encoding_dict_list[0]            \n",
    "        return encoding_dict\n",
    "    \n",
    "    def word_index_dict(self, sorted_words):\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "    \n",
    "        for i, word in enumerate(sorted_words):\n",
    "            word_to_index[word] = i+1\n",
    "            index_to_word[i+1] = word\n",
    "    \n",
    "        index_to_word[1846] = 'startseq'\n",
    "        word_to_index['startseq'] = 1846\n",
    "    \n",
    "        index_to_word[1847] = 'endseq'\n",
    "        word_to_index['endseq'] = 1847\n",
    "    \n",
    "        vocab_size = len(word_to_index) + 1\n",
    "        return word_to_index, index_to_word, vocab_size\n",
    "\n",
    "    def get_embedding_matrix(self, vocab_size, word_to_index):\n",
    "        embedding_index = {}\n",
    "        f = open('D:/Deployment projects/Image Captioning/docs/glove.6B.50d.txt', encoding = 'utf8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_embedding = np.array(values[1:], dtype = 'float')\n",
    "            embedding_index[word] = word_embedding\n",
    "        f.close()\n",
    "        \n",
    "        emb_dim = 50\n",
    "        matrix = np.zeros((vocab_size, emb_dim))\n",
    "        for word, idx in word_to_index.items():\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            \n",
    "            if embedding_vector is not None:\n",
    "                matrix[idx] = embedding_vector\n",
    "        return matrix\n",
    "    \n",
    "\n",
    "class ML:\n",
    "    \n",
    "    def data_generator(self, train_description, encoding_data, word_to_index, max_length, vocab_size, batch_size):\n",
    "        x1, x2, y = [], [], []\n",
    "        n = 0\n",
    "        while True:\n",
    "            for key, desc_list in train_description.items():\n",
    "                n+=1\n",
    "                photo = encoding_data[key]\n",
    "                for desc in desc_list:\n",
    "                    seq = [word_to_index[word] for word in desc.split(\" \") if word in word_to_index]\n",
    "                    for i in range(1, len(seq)):\n",
    "                        xi = seq[0:i]\n",
    "                        yi = seq[i]\n",
    "                        \n",
    "                        # 0 denoting padding word\n",
    "                        xi = tf.keras.preprocessing.sequence.pad_sequences([xi],maxlen = max_length, value = 0, padding = 'post')[0]\n",
    "                        yi = to_categorical([yi], num_classes = vocab_size)[0]\n",
    "                        \n",
    "                        x1.append(photo)\n",
    "                        x2.append(xi)\n",
    "                        y.append(yi)\n",
    "            \n",
    "                    if n == batch_size:\n",
    "                        yield [[np.array(x1),np.array(x2)],np.array(y)]\n",
    "                        x1, x2, y = [], [], []\n",
    "                        n = 0    \n",
    "        \n",
    "    def model_building_and_training(self, max_length, vocab_size, embedding_matrix, train_description, encoded_train_data, word_to_index, epochs = 20, batch_size = 3):\n",
    "    \n",
    "    # For processing image features\n",
    "        input_image_features = Input(shape=(2048,))\n",
    "        inp_img1 = Dropout(0.3)(input_image_features)\n",
    "        inp_img2 = Dense(256, activation = 'relu')(inp_img1)\n",
    "    \n",
    "        # For processing captions\n",
    "        input_captions = Input(shape = (max_length,))\n",
    "        inp_cap1 = Embedding(input_dim = vocab_size, output_dim = 50, mask_zero = True)(input_captions)\n",
    "        inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "        inp_cap3 = LSTM(256)(inp_cap2)\n",
    "    \n",
    "        # For combining inputs to get output\n",
    "        decoder1 = add([inp_img2, inp_cap3])\n",
    "        decoder2 = Dense(256, activation = 'relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation  = 'softmax')(decoder2)\n",
    "    \n",
    "        # Combined model\n",
    "        model = Model(inputs = [input_image_features, input_captions], outputs = outputs)\n",
    "    \n",
    "        # initializing weightage in embedded layer\n",
    "        model.layers[2].set_weights([embedding_matrix])\n",
    "        model.layers[2].trainable = False\n",
    "    \n",
    "        # compile the model\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "        \n",
    "        steps = len(train_description)//batch_size\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            generator = self.data_generator(train_description, encoded_train_data, word_to_index, max_length, vocab_size, batch_size)\n",
    "            model.fit_generator(generator, epochs = 1, steps_per_epoch = steps, verbose = 1)\n",
    "            model.save('D://Deployment projects//Image Captioning//results//model weights/model_'+str(i)+'.h5')\n",
    "\n",
    "\n",
    "    def required_file_generator(self):\n",
    "        \n",
    "        dataingest_obj = data_ingest()\n",
    "    \n",
    "        img_caption_dict = dataingest_obj.caption_read('D://Deployment projects//Image Captioning//Data//captions.txt')\n",
    "        dataingest_obj.write_clean_captions('D://Deployment projects//Image Captioning//Generated files//caption_for_image.txt','D://Deployment projects//Image Captioning//Data//captions.txt')\n",
    "        vocab, final_sorted_words_with_threshold = dataingest_obj.read_data_for_vocab('D://Deployment projects//Image Captioning//Generated files//caption_for_image.txt')\n",
    "        train_data, test_data, dev_data = dataingest_obj.train_test_dev_data('D://Deployment projects//Image Captioning//Data//train.txt','D://Deployment projects//Image Captioning//Data//test.txt','D://Deployment projects//Image Captioning//Data//dev.txt')\n",
    "        train_description, max_length = dataingest_obj.train_descriptions(train_data, img_caption_dict)\n",
    "        encoded_train_data = dataingest_obj.encoding_dict_from_pickle('D://Deployment projects//Image Captioning//Generated files//encode_train_features.pkl')\n",
    "        encoded_test_data = dataingest_obj.encoding_dict_from_pickle('D://Deployment projects//Image Captioning//Generated files//encode_test_features.pkl')\n",
    "        dataingest_obj.writing_encoded_data('encode_train_features.pkl',train_data,'D://Deployment projects//Image Captioning//Data//Images//')\n",
    "        dataingest_obj.writing_encoded_data('encode_test_features.pkl',test_data,'D://Deployment projects//Image Captioning//Data//Images//')\n",
    "        word_to_index, index_to_word, vocab_size = dataingest_obj.word_index_dict(final_sorted_words_with_threshold)\n",
    "        embedding_matrix = dataingest_obj.get_embedding_matrix(vocab_size, word_to_index)\n",
    "        self.model_building_and_training(max_length, vocab_size, embedding_matrix, train_description, encoded_train_data, word_to_index, epochs = 20, batch_size = 3)\n",
    "        \n",
    "\n",
    "\n",
    "# ML_obj = ML()\n",
    "# ML_obj.required_file_generator()\n",
    "\n",
    "\n",
    "\n",
    "class predict_it:\n",
    "    \n",
    "    def predict_caption(self,img_path):\n",
    "        \n",
    "        dataingest_obj = data_ingest()\n",
    "        \n",
    "        img_caption_dict = dataingest_obj.caption_read('../Data/captions.txt')\n",
    "        vocab, final_sorted_words_with_threshold = dataingest_obj.read_data_for_vocab('../Generated files/caption_for_image.txt')\n",
    "        train_data, test_data, dev_data = dataingest_obj.train_test_dev_data('../Data/train.txt','../Data/test.txt','../Data/dev.txt')\n",
    "        train_description, max_length = dataingest_obj.train_descriptions(train_data, img_caption_dict)\n",
    "        word_to_index, index_to_word, vocab_size = dataingest_obj.word_index_dict(final_sorted_words_with_threshold)\n",
    "        \n",
    "        photo = dataingest_obj.encode_img(img_path)\n",
    "        photo = photo.reshape(1,2048)\n",
    "        in_text = 'startseq'\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            \n",
    "            model_ml = load_model('../results/model weights/model_19.h5')\n",
    "            sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "            sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen = max_length, padding = 'post')\n",
    "            \n",
    "            ypred = model_ml.predict([photo, sequence])\n",
    "            ypred = ypred.argmax()\n",
    "            \n",
    "            word = index_to_word[ypred]\n",
    "            in_text += (' ' + word)\n",
    "            \n",
    "            if word == 'endseq':\n",
    "                break\n",
    "        \n",
    "        final_caption = in_text.split()[1:-1]\n",
    "        final_caption = ' '.join(final_caption)\n",
    "        \n",
    "        return final_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "964ddefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_obj = predict_it()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524de01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025E0492A280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025E046A2430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'man is climbing up rock'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_obj.predict_caption('../Data/Images/3582685410_05315a15b8.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f41fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
